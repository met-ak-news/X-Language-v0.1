# Simple tokenizer stub
def tokenize(text: str):
    return text.split()
